"use strict";
var __createBinding = (this && this.__createBinding) || (Object.create ? (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    var desc = Object.getOwnPropertyDescriptor(m, k);
    if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
      desc = { enumerable: true, get: function() { return m[k]; } };
    }
    Object.defineProperty(o, k2, desc);
}) : (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    o[k2] = m[k];
}));
var __setModuleDefault = (this && this.__setModuleDefault) || (Object.create ? (function(o, v) {
    Object.defineProperty(o, "default", { enumerable: true, value: v });
}) : function(o, v) {
    o["default"] = v;
});
var __importStar = (this && this.__importStar) || function (mod) {
    if (mod && mod.__esModule) return mod;
    var result = {};
    if (mod != null) for (var k in mod) if (k !== "default" && Object.prototype.hasOwnProperty.call(mod, k)) __createBinding(result, mod, k);
    __setModuleDefault(result, mod);
    return result;
};
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.AnalyticDBVectorStore = void 0;
const uuid = __importStar(require("uuid"));
const pg_1 = __importDefault(require("pg"));
const pg_copy_streams_1 = require("pg-copy-streams");
const promises_1 = require("node:stream/promises");
const node_stream_1 = require("node:stream");
const base_js_1 = require("./base.cjs");
const document_js_1 = require("../document.cjs");
const _LANGCHAIN_DEFAULT_EMBEDDING_DIM = 1536;
const _LANGCHAIN_DEFAULT_COLLECTION_NAME = "langchain_document";
class AnalyticDBVectorStore extends base_js_1.VectorStore {
    _vectorstoreType() {
        return "analyticdb";
    }
    constructor(embeddings, args) {
        super(embeddings, args);
        Object.defineProperty(this, "pool", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "embeddingDimension", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "collectionName", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "preDeleteCollection", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "isCreateCollection", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        this.pool = new pg_1.default.Pool({
            host: args.connectionOptions.host,
            port: args.connectionOptions.port,
            database: args.connectionOptions.database,
            user: args.connectionOptions.user,
            password: args.connectionOptions.password,
        });
        this.embeddingDimension =
            args.embeddingDimension || _LANGCHAIN_DEFAULT_EMBEDDING_DIM;
        this.collectionName =
            args.collectionName || _LANGCHAIN_DEFAULT_COLLECTION_NAME;
        this.preDeleteCollection = args.preDeleteCollection || false;
    }
    async end() {
        return this.pool.end();
    }
    async createTableIfNotExists() {
        const client = await this.pool.connect();
        try {
            await client.query("BEGIN");
            // Create the table if it doesn't exist
            await client.query(`
        CREATE TABLE IF NOT EXISTS ${this.collectionName} (
          id TEXT PRIMARY KEY DEFAULT NULL,
          embedding REAL[],
          document TEXT,
          metadata JSON
        );
      `);
            // Check if the index exists
            const indexName = `${this.collectionName}_embedding_idx`;
            const indexQuery = `
        SELECT 1
        FROM pg_indexes
        WHERE indexname = '${indexName}';
      `;
            const result = await client.query(indexQuery);
            // Create the index if it doesn't exist
            if (result.rowCount === 0) {
                const indexStatement = `
          CREATE INDEX ${indexName}
          ON ${this.collectionName} USING ann(embedding)
          WITH (
            "dim" = ${this.embeddingDimension},
            "hnsw_m" = 100
          );
        `;
                await client.query(indexStatement);
            }
            await client.query("COMMIT");
        }
        catch (err) {
            await client.query("ROLLBACK");
            throw err;
        }
        finally {
            client.release();
        }
    }
    async deleteCollection() {
        const dropStatement = `DROP TABLE IF EXISTS ${this.collectionName};`;
        await this.pool.query(dropStatement);
    }
    async createCollection() {
        if (this.preDeleteCollection) {
            await this.deleteCollection();
        }
        await this.createTableIfNotExists();
        this.isCreateCollection = true;
    }
    async addDocuments(documents) {
        const texts = documents.map(({ pageContent }) => pageContent);
        return this.addVectors(await this.embeddings.embedDocuments(texts), documents);
    }
    async addVectors(vectors, documents) {
        if (vectors.length === 0) {
            return;
        }
        if (vectors.length !== documents.length) {
            throw new Error(`Vectors and documents must have the same length`);
        }
        if (vectors[0].length !== this.embeddingDimension) {
            throw new Error(`Vectors must have the same length as the number of dimensions (${this.embeddingDimension})`);
        }
        if (!this.isCreateCollection) {
            await this.createCollection();
        }
        const client = await this.pool.connect();
        try {
            const chunkSize = 500;
            const chunksTableData = [];
            for (let i = 0; i < documents.length; i += 1) {
                chunksTableData.push({
                    id: uuid.v4(),
                    embedding: vectors[i],
                    document: documents[i].pageContent,
                    metadata: documents[i].metadata,
                });
                // Execute the batch insert when the batch size is reached
                if (chunksTableData.length === chunkSize) {
                    const rs = new node_stream_1.Readable();
                    let currentIndex = 0;
                    rs._read = function () {
                        if (currentIndex === chunkSize) {
                            rs.push(null);
                        }
                        else {
                            const data = chunksTableData[currentIndex];
                            rs.push(`${data.id}\t{${data.embedding.join(",")}}\t${data.document}\t${JSON.stringify(data.metadata)}\n`);
                            currentIndex += 1;
                        }
                    };
                    const ws = client.query((0, pg_copy_streams_1.from)(`COPY ${this.collectionName}(id, embedding, document, metadata) FROM STDIN`));
                    await (0, promises_1.pipeline)(rs, ws);
                    // Clear the chunksTableData list for the next batch
                    chunksTableData.length = 0;
                }
            }
            // Insert any remaining records that didn't make up a full batch
            if (chunksTableData.length > 0) {
                const rs = new node_stream_1.Readable();
                let currentIndex = 0;
                rs._read = function () {
                    if (currentIndex === chunksTableData.length) {
                        rs.push(null);
                    }
                    else {
                        const data = chunksTableData[currentIndex];
                        rs.push(`${data.id}\t{${data.embedding.join(",")}}\t${data.document}\t${JSON.stringify(data.metadata)}\n`);
                        currentIndex += 1;
                    }
                };
                const ws = client.query((0, pg_copy_streams_1.from)(`COPY ${this.collectionName}(id, embedding, document, metadata) FROM STDIN`));
                await (0, promises_1.pipeline)(rs, ws);
            }
        }
        finally {
            client.release();
        }
    }
    async similaritySearchVectorWithScore(query, k, filter) {
        if (!this.isCreateCollection) {
            await this.createCollection();
        }
        let filterCondition = "";
        const filterEntries = filter ? Object.entries(filter) : [];
        if (filterEntries.length > 0) {
            const conditions = filterEntries.map((_, index) => `metadata->>$${2 * index + 3} = $${2 * index + 4}`);
            filterCondition = `WHERE ${conditions.join(" AND ")}`;
        }
        const sqlQuery = `
      SELECT *, l2_distance(embedding, $1::real[]) AS distance
      FROM ${this.collectionName}
      ${filterCondition}
      ORDER BY embedding <-> $1
      LIMIT $2;
    `;
        // Execute the query and fetch the results
        const { rows } = await this.pool.query(sqlQuery, [
            query,
            k,
            ...filterEntries.flatMap(([key, value]) => [key, value]),
        ]);
        const result = rows.map((row) => [
            new document_js_1.Document({ pageContent: row.document, metadata: row.metadata }),
            row.distance,
        ]);
        return result;
    }
    static async fromTexts(texts, metadatas, embeddings, dbConfig) {
        const docs = [];
        for (let i = 0; i < texts.length; i += 1) {
            const metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;
            const newDoc = new document_js_1.Document({
                pageContent: texts[i],
                metadata,
            });
            docs.push(newDoc);
        }
        return AnalyticDBVectorStore.fromDocuments(docs, embeddings, dbConfig);
    }
    static async fromDocuments(docs, embeddings, dbConfig) {
        const instance = new this(embeddings, dbConfig);
        await instance.addDocuments(docs);
        return instance;
    }
    static async fromExistingIndex(embeddings, dbConfig) {
        const instance = new this(embeddings, dbConfig);
        await instance.createCollection();
        return instance;
    }
}
exports.AnalyticDBVectorStore = AnalyticDBVectorStore;
